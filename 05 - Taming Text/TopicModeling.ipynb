{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA\n",
    "\n",
    "The algorithm we want to apply to do topic modeling is known as **Latent Dirichlet Allocation** (for an exhaustive reference see [here](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)). This approach is mainly statistics-based, since a *Dirichlet prior* distribution is assumed for the training corpus. Multiple runs of the LDA on the same bunch of documents will provide slighlty different results, but once the final model is saved, its application to new corpora will be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import at first the cleaned body text from a csv file, build up during the previous sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hillary-clinton-emails/sentimentEmails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentiment', 'SemiProcessedData', 'FullSemiProcessedData',\n",
       "       'ProcessedData'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we are interested in now is *ProcessedData*, where the cleaned textual information has been extracted. We check the eventual presence of NaN values in the dataset before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6453\n",
       "True        3\n",
       "Name: ProcessedData, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedData.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are NaNs left, we will wipe them out when building the corpus, as they cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define a list of stopwords to be wiped out from the documents; this words are typical English language stopwords or some trivial expressions, such as mail vocabulary tokens ('fw'), some numbers (which often prevent from a clear understanding of the underlying message), basic words ('get') and well as punctuation symbols. Numbers which are higher than 1899 represent likely years in dates and thus may contain useful information. Thereafter we define a list *documents* containing the split words of any text in *ProcessedData*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the documents from trivial recurrent words and from punctuation symbols\n",
    "\n",
    "punctuation_symbols = ['.',',',';',':','-','•','\"',\"'\",'?','!','@','#','/','*','+','(',')','—','{','}',\n",
    "                      '.\"',',\"','),','(,','<','>','%','&','$','---','----','-----','------','[',']',\n",
    "                      '■','--','...','://',').']\n",
    "trivial_words = ['us','fyi','fw','get']\n",
    "\n",
    "numbs = range(1900)\n",
    "numbers = [str(n) for n in numbs]\n",
    "numbers.insert(0,'00')\n",
    "\n",
    "stoplist = list(set(trivial_words).union(set(punctuation_symbols).union(set(numbers))))\n",
    "\n",
    "# apply the stoplist to each document in RawText\n",
    "documents = [[word for word in text.lower().split() if word not in stoplist and len(word)>1] # wipe out single letters\n",
    "            for text in df.ProcessedData.dropna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the *gensim* library and define a **dictionary**, which matches any word in each text with a numeric ID; notice that the documents are treated as *bows* (numeric vectors); the output of this operation is the **corpus** we will perform analysis on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a dictionary to associate ad Id to each token and build the corpus\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Latent Dirichlet Allocation using the dictionary and the corpus. The parameter *no_topics* defines the number of topics the algorithm must identify throughout the corpus. The higher it is, the more specific the returned topics will appear. Here we have chosen no_topics = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an lda model using the previously defined dictionary\n",
    "no_topics = 20\n",
    "passes = 3\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics=no_topics,passes=passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method of the LdaModel class allows to visualize the selected topics as a collection of (word,probability) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '0.023*\"today\" + 0.015*\"blair\" + 0.014*\"tomorrow\" + 0.012*\"shuttle\" + 0.012*\"confirm\" + 0.010*\"email\" + 0.008*\"time\" + 0.008*\"tony\" + 0.007*\"ops\" + 0.007*\"leave\"'),\n",
       " (8,\n",
       "  '0.013*\"mr\" + 0.009*\"obama\" + 0.007*\"tell\" + 0.006*\"take\" + 0.005*\"day\" + 0.005*\"mayor\" + 0.005*\"try\" + 0.005*\"seem\" + 0.005*\"like\" + 0.005*\"good\"'),\n",
       " (13,\n",
       "  '0.018*\"tomorrow\" + 0.014*\"holbrooke\" + 0.012*\"b1\" + 0.012*\"schedule\" + 0.012*\"discuss\" + 0.010*\"tonight\" + 0.008*\"final\" + 0.008*\"ask\" + 0.008*\"follow\" + 0.008*\"today\"'),\n",
       " (2,\n",
       "  '0.028*\"speech\" + 0.025*\"doc\" + 0.023*\"draft\" + 0.021*\"state\" + 0.015*\"gaza\" + 0.015*\"2015\" + 0.014*\"fax\" + 0.013*\"thx\" + 0.011*\"case\" + 0.011*\"house\"'),\n",
       " (0,\n",
       "  '0.012*\"part\" + 0.011*\"release\" + 0.009*\"b6\" + 0.009*\"happy\" + 0.009*\"bill\" + 0.008*\"week\" + 0.008*\"update\" + 0.008*\"boehner\" + 0.008*\"last\" + 0.008*\"great\"'),\n",
       " (14,\n",
       "  '0.015*\"israel\" + 0.011*\"israeli\" + 0.008*\"netanyahu\" + 0.008*\"american\" + 0.007*\"iran\" + 0.007*\"right\" + 0.006*\"bibi\" + 0.006*\"jewish\" + 0.006*\"come\" + 0.005*\"point\"'),\n",
       " (6,\n",
       "  '0.016*\"state\" + 0.010*\"vote\" + 0.007*\"support\" + 0.007*\"madame\" + 0.007*\"receive\" + 0.007*\"agreement\" + 0.007*\"message\" + 0.006*\"uk\" + 0.006*\"mcconnell\" + 0.006*\"mail\"'),\n",
       " (17,\n",
       "  '0.022*\"obama\" + 0.010*\"percent\" + 0.009*\"republican\" + 0.009*\"2010\" + 0.009*\"president\" + 0.008*\"democrats\" + 0.008*\"republicans\" + 0.008*\"vote\" + 0.007*\"palin\" + 0.006*\"voters\"'),\n",
       " (18,\n",
       "  '0.021*\"email\" + 0.011*\"melanne\" + 0.011*\"yes\" + 0.009*\"another\" + 0.009*\"send\" + 0.008*\"still\" + 0.008*\"verveer\" + 0.008*\"sorry\" + 0.008*\"language\" + 0.007*\"issue\"'),\n",
       " (1,\n",
       "  '0.017*\"force\" + 0.016*\"mcchrystal\" + 0.016*\"national\" + 0.013*\"airport\" + 0.012*\"general\" + 0.010*\"air\" + 0.009*\"secretary\" + 0.009*\"senior\" + 0.009*\"military\" + 0.007*\"washington\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display 10 words per topic as default\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the selected topics in one glance, we put in a list all the words defining a certain topic, we join them in a unique string and assign this one to a list called *topics*. The resulting topics are then printed by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1: part  release  b6  happy  bill  week  update  boehner  last  great  read  back',\n",
       " 'Topic 2: force  mcchrystal  national  airport  general  air  secretary  senior  military  washington  pentagon  andrews',\n",
       " 'Topic 3: speech  doc  draft  state  gaza  2015  fax  thx  case  house  aipac  ok',\n",
       " 'Topic 4: nuclear  state  unite  people  support  conflict  include  start  government  world  international  force',\n",
       " 'Topic 5: state  afghanistan  department  war  government  afghan  diplomats  force  women  taliban  attack  support',\n",
       " 'Topic 6: time  public  clinton  president  even  tell  cable  group  book  day  women  washington',\n",
       " 'Topic 7: state  vote  support  madame  receive  agreement  message  uk  mcconnell  mail  local  palau',\n",
       " 'Topic 8: diplomacy  treaty  development  state  send  department  foreign  diplomats  good  original  richard  via',\n",
       " 'Topic 9: mr  obama  tell  take  day  mayor  try  seem  like  good  big  back',\n",
       " 'Topic 10: today  blair  tomorrow  shuttle  confirm  email  time  tony  ops  leave  office  leahy',\n",
       " 'Topic 11: secretary  office  room  state  department  conference  arrive  en  depart  route  private  time',\n",
       " 'Topic 12: print  sid  pls  hrc  memo  statement  pis  copy  thx  hillary  docx  unfavorable',\n",
       " 'Topic 13: state  gov  2010  com  cheryl  clintonemail  b6  hrod17  mill  sullivan  huma  monday',\n",
       " 'Topic 14: tomorrow  holbrooke  b1  schedule  discuss  tonight  final  ask  follow  today  report  trip',\n",
       " 'Topic 15: israel  israeli  netanyahu  american  iran  right  bibi  jewish  come  point  like  time',\n",
       " 'Topic 16: list  haiti  ok  mtg  report  confidential  secure  add  mins  involvement  book  ellen',\n",
       " 'Topic 17: party  right  labour  bloomberg  david  sid  time  ed  company  cameron  election  write',\n",
       " 'Topic 18: obama  percent  republican  2010  president  democrats  republicans  vote  palin  voters  care  poll',\n",
       " 'Topic 19: email  melanne  yes  another  send  still  verveer  sorry  language  issue  quick  come',\n",
       " 'Topic 20: state  obama  peace  israel  president  clinton  administration  palestinians  washington  time  travel  negotiations']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics here will be a list of strings\n",
    "no_words = 12 # number of words per topic to be printed\n",
    "topics = []\n",
    "for num in range(no_topics):\n",
    "    topic_prob = lda.show_topic(num,no_words)\n",
    "    topic = []\n",
    "    for word in range(len(topic_prob)):\n",
    "        topic.append(topic_prob[word][0])\n",
    "    topic = '  '.join(topic)\n",
    "    topics.append(('Topic '+str(num+1)+': '+topic))\n",
    "topics"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
