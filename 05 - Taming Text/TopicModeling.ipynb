{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import at first the cleaned body text from a csv file, build up during the previous sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hillary-clinton-emails/sentimentEmails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentiment', 'SemiProcessedData', 'FullSemiProcessedData',\n",
       "       'ProcessedData'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we are interested in now is *ProcessedData*, where the cleaned textual information has been extracted. We check the eventual presence of NaN values in the dataset before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6453\n",
       "True        3\n",
       "Name: ProcessedData, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedData.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are NaNs left, we will wipe them out when building the corpus, as they cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define a list of stopwords to be wiped out from the documents; this words are typical English language stopwords or some trivial expressions, such as mail vocabulary tokens ('fw', 're'), \"small\" numbers (which often prevent from a clear understanding of the underlying message), basic words ('get','would') and well as punctuation symbols. \"High\" numbers represent likely years in dates and thus may contain useful information. Thereafter we define a list *documents* containing the split words of any text in *ProcessedData*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the documents from trivial recurrent words and from punctuation symbols\n",
    "\n",
    "punctuation_symbols = ['.',',',';',':','-','•','\"',\"'\",'?','!','@','#','/','*','+','(',')','—','{','}',\n",
    "                      '.\"',',\"','),','(,','<','>','%','&','$','---','----','-----','------','[',']',\n",
    "                      '■','--','...','://']\n",
    "trivial_words = ['u','w','h','j','us','fyi','would','fw','get']\n",
    "\n",
    "numbs = range(100)\n",
    "numbers = [str(n) for n in numbs]\n",
    "numbers = list(set(numbers).union(set(['00'])))\n",
    "\n",
    "stoplist = list(set(trivial_words).union(set(punctuation_symbols).union(set(numbers))))\n",
    "\n",
    "# apply the stoplist to each document in RawText\n",
    "documents = [[word for word in text.lower().split() if word not in stoplist]\n",
    "            for text in df.ProcessedData.dropna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the *gensim* library and define a **dictionary**, which matches any word in each text with a numeric ID; notice that the documents are treated as *bows* (numeric vectors); the output of this operation is the **corpus** we will perform analysis on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:840: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Utente\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# define a dictionary to associate ad Id to each token and build the corpus\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Latent Dirichlet Allocation using the dictionary and the corpus. The parameter *no_topics* defines the number of topics the algorithm must identify throughout the corpus. The higher it is, the more specific the returned topics will appear. Here we have chosen no_topics = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an lda model using the previously defined dictionary\n",
    "no_topics = 20\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics=no_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method of the LdaModel class allows to visualize the selected topics as a collection of (word,probability) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  '0.021*\"ok\" + 0.011*\"cameron\" + 0.010*\"sid\" + 0.009*\"ops\" + 0.008*\"today\" + 0.007*\"email\" + 0.007*\"miliband\" + 0.007*\"arizona\" + 0.007*\"mins\" + 0.006*\"waldorf\"'),\n",
       " (11,\n",
       "  '0.007*\"afghanistan\" + 0.007*\"mcchrystal\" + 0.007*\"war\" + 0.006*\"military\" + 0.006*\"civilian\" + 0.006*\"force\" + 0.005*\"general\" + 0.005*\"nuclear\" + 0.005*\"state\" + 0.004*\"strategy\"'),\n",
       " (15,\n",
       "  '0.021*\"mr\" + 0.012*\"company\" + 0.009*\"back\" + 0.008*\"wjc\" + 0.006*\"afghan\" + 0.006*\"tomorrow\" + 0.005*\"yes\" + 0.005*\"2010\" + 0.004*\"state\" + 0.004*\"last\"'),\n",
       " (8,\n",
       "  '0.029*\"state\" + 0.027*\"gov\" + 0.027*\"2010\" + 0.014*\"cheryl\" + 0.013*\"mill\" + 0.011*\"com\" + 0.011*\"clintonemail\" + 0.010*\"hrod17\" + 0.008*\"millscd\" + 0.007*\"monday\"'),\n",
       " (16,\n",
       "  '0.012*\"blair\" + 0.010*\"sid\" + 0.010*\"ashton\" + 0.008*\"sullivan\" + 0.008*\"sunday\" + 0.007*\"confidential\" + 0.006*\"beck\" + 0.006*\"book\" + 0.006*\"2010\" + 0.005*\"info\"'),\n",
       " (19,\n",
       "  '0.006*\"2010\" + 0.006*\"point\" + 0.006*\"haiti\" + 0.005*\"support\" + 0.005*\"people\" + 0.004*\"obama\" + 0.004*\"take\" + 0.004*\"women\" + 0.003*\"abu\" + 0.003*\"vote\"'),\n",
       " (6,\n",
       "  '0.008*\"state\" + 0.006*\"party\" + 0.006*\"obama\" + 0.005*\"2010\" + 0.005*\"house\" + 0.004*\"president\" + 0.004*\"republicans\" + 0.004*\"republican\" + 0.004*\"time\" + 0.004*\"people\"'),\n",
       " (7,\n",
       "  '0.051*\"secretary\" + 0.034*\"office\" + 0.025*\"room\" + 0.017*\"state\" + 0.015*\"conference\" + 0.014*\"time\" + 0.012*\"department\" + 0.012*\"treaty\" + 0.011*\"private\" + 0.010*\"arrive\"'),\n",
       " (0,\n",
       "  '0.019*\"state\" + 0.010*\"unite\" + 0.008*\"american\" + 0.006*\"diplomacy\" + 0.005*\"department\" + 0.005*\"conflict\" + 0.005*\"security\" + 0.005*\"diplomats\" + 0.004*\"public\" + 0.004*\"government\"'),\n",
       " (12,\n",
       "  '0.009*\"nuclear\" + 0.007*\"state\" + 0.007*\"send\" + 0.007*\"tomorrow\" + 0.005*\"time\" + 0.005*\"speech\" + 0.005*\"note\" + 0.005*\"draft\" + 0.005*\"pls\" + 0.005*\"thank\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the selected topics in one glance, we put in a list all the words defining a certain topic, we join them in a unique string and assign this one to a list called *topics*. The resulting topics are then printed by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state  unite  american  diplomacy  department  conflict  security  diplomats  public  government',\n",
       " 'ok  cameron  sid  ops  today  email  miliband  arizona  mins  waldorf',\n",
       " 'time  obama  party  president  mr  washington  israel  policy  state  mayor',\n",
       " 'senate  party  vote  like  read  tell  israeli  russia  israel  question',\n",
       " 'bloomberg  report  qddr  give  holbrooke  tomorrow  speech  good  come  could',\n",
       " 'force  state  taliban  government  conflict  war  kabul  fund  afghanistan  military',\n",
       " 'state  party  obama  2010  house  president  republicans  republican  time  people',\n",
       " 'secretary  office  room  state  conference  time  department  treaty  private  arrive',\n",
       " 'state  gov  2010  cheryl  mill  com  clintonemail  hrod17  millscd  monday',\n",
       " 'iran  china  state  diplomats  world  iranian  border  federal  unite  government',\n",
       " 'prevention  state  israel  2010  american  sanction  sid  anytime  iran  may',\n",
       " 'afghanistan  mcchrystal  war  military  civilian  force  general  nuclear  state  strategy',\n",
       " 'nuclear  state  send  tomorrow  time  speech  note  draft  pls  thank',\n",
       " 'state  office  secretary  department  airport  diplomacy  arrive  route  en  gov',\n",
       " 'right  israel  war  issue  bibi  people  support  israeli  netanyahu  like',\n",
       " 'mr  company  back  wjc  afghan  tomorrow  yes  2010  state  last',\n",
       " 'blair  sid  ashton  sullivan  sunday  confidential  beck  book  2010  info',\n",
       " 'clinton  b6  part  b  release  b1  obama  leak  state  policy',\n",
       " 'secretary  office  bill  state  treaty  president  time  conference  foreign  sid',\n",
       " '2010  point  haiti  support  people  obama  take  women  abu  vote']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics here will be a list of strings\n",
    "topics = []\n",
    "for num in range(no_topics):\n",
    "    topic_prob = lda.show_topic(num)\n",
    "    topic = []\n",
    "    for word in range(len(topic_prob)):\n",
    "        topic.append(topic_prob[word][0])\n",
    "    topic = '  '.join(topic)\n",
    "    topics.append(topic)\n",
    "topics"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
