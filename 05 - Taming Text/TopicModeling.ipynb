{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA\n",
    "\n",
    "The algorithm we want to apply to do topic modeling is known as **Latent Dirichlet Allocation** (for an exhaustive reference see [here](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)). This approach is mainly statistics-based, since a *Dirichlet prior* distribution is assumed for the training corpus. Multiple runs of the LDA on the same bunch of documents will provide slighlty different results, but once the final model is saved, its application to new corpora will be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import at first the cleaned body text from a csv file, build up during the previous sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hillary-clinton-emails/sentimentEmails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SentimentSIA', 'SentimentLHL', 'SemiProcessedData',\n",
       "       'FullSemiProcessedData', 'ProcessedData'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we are interested in now is *ProcessedData*, where the cleaned textual information has been extracted. We check the eventual presence of NaN values in the dataset before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6453\n",
       "True        3\n",
       "Name: ProcessedData, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedData.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are NaNs left, we will wipe them out when building the corpus, as they cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define a list of stopwords to be wiped out from the documents; this words are typical English language stopwords or some trivial expressions, such as mail vocabulary tokens ('fw'), some numbers (which often prevent from a clear understanding of the underlying message), basic words ('get') and well as punctuation symbols. Numbers which are higher than 1899 represent likely years in dates and thus may contain useful information. Thereafter we define a list *documents* containing the split words of any text in *ProcessedData*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the documents from trivial recurrent words and from punctuation symbols\n",
    "\n",
    "punctuation_symbols = ['.',',',';',':','-','•','\"',\"'\",'?','!','@','#','/','*','+','(',')','—','{','}',\n",
    "                      '.\"',',\"','),','(,','<','>','%','&','$','---','----','-----','------','[',']',\n",
    "                      '■','--','...','://',').']\n",
    "trivial_words = ['us','fyi','fw','get']\n",
    "\n",
    "numbs = range(1900)\n",
    "numbers = [str(n) for n in numbs]\n",
    "numbers.insert(0,'00')\n",
    "\n",
    "stoplist = list(set(trivial_words).union(set(punctuation_symbols).union(set(numbers))))\n",
    "\n",
    "# apply the stoplist to each document in RawText\n",
    "documents = [[word for word in text.lower().split() if word not in stoplist and len(word)>1] # wipe out single letters\n",
    "            for text in df.ProcessedData.dropna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the *gensim* library and define a **dictionary**, which matches any word in each text with a numeric ID; notice that the documents are treated as *bows* (numeric vectors); the output of this operation is the **corpus** we will perform analysis on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# define a dictionary to associate ad Id to each token and build the corpus\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Latent Dirichlet Allocation using the dictionary and the corpus. The parameter *no_topics* defines the number of topics the algorithm must identify throughout the corpus. The higher it is, the more specific the returned topics will appear. Here we have chosen no_topics = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an lda model using the previously defined dictionary\n",
    "no_topics = 20\n",
    "passes = 3\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics=no_topics,passes=passes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method of the LdaModel class allows to visualize the selected topics as a collection of (word,probability) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '0.017*\"mtg\" + 0.017*\"air\" + 0.016*\"andrews\" + 0.012*\"base\" + 0.012*\"house\" + 0.012*\"force\" + 0.011*\"mitchell\" + 0.011*\"white\" + 0.010*\"state\" + 0.009*\"nations\"'),\n",
       " (16,\n",
       "  '0.015*\"mayor\" + 0.012*\"melanne\" + 0.011*\"washington\" + 0.010*\"verveer\" + 0.009*\"dc\" + 0.008*\"chicago\" + 0.008*\"city\" + 0.007*\"nw\" + 0.007*\"amendment\" + 0.007*\"district\"'),\n",
       " (4,\n",
       "  '0.032*\"airport\" + 0.015*\"laguardia\" + 0.011*\"office\" + 0.010*\"national\" + 0.009*\"arrive\" + 0.009*\"route\" + 0.009*\"en\" + 0.009*\"york\" + 0.009*\"leahy\" + 0.009*\"conf\"'),\n",
       " (14,\n",
       "  '0.017*\"bloomberg\" + 0.012*\"company\" + 0.010*\"china\" + 0.008*\"koch\" + 0.006*\"climate\" + 0.006*\"fund\" + 0.006*\"change\" + 0.006*\"per\" + 0.005*\"praise\" + 0.005*\"asia\"'),\n",
       " (17,\n",
       "  '0.011*\"time\" + 0.009*\"discuss\" + 0.009*\"back\" + 0.009*\"tomorrow\" + 0.009*\"let\" + 0.008*\"qddr\" + 0.008*\"like\" + 0.008*\"ask\" + 0.008*\"happy\" + 0.007*\"come\"'),\n",
       " (1,\n",
       "  '0.017*\"russia\" + 0.016*\"email\" + 0.009*\"expeditionary\" + 0.009*\"saddam\" + 0.009*\"prevention\" + 0.009*\"send\" + 0.008*\"postconflict\" + 0.008*\"limit\" + 0.007*\"tony\" + 0.006*\"traffic\"'),\n",
       " (19,\n",
       "  '0.025*\"party\" + 0.017*\"labour\" + 0.014*\"ok\" + 0.011*\"david\" + 0.009*\"cameron\" + 0.008*\"miliband\" + 0.007*\"tomorrow\" + 0.007*\"election\" + 0.006*\"move\" + 0.006*\"members\"'),\n",
       " (12,\n",
       "  '0.021*\"update\" + 0.018*\"holbrooke\" + 0.017*\"treaty\" + 0.010*\"try\" + 0.010*\"start\" + 0.010*\"speech\" + 0.009*\"defenses\" + 0.009*\"leak\" + 0.009*\"russian\" + 0.008*\"gelb\"'),\n",
       " (6,\n",
       "  '0.019*\"haiti\" + 0.018*\"cable\" + 0.015*\"pakistan\" + 0.011*\"report\" + 0.010*\"sudan\" + 0.010*\"letter\" + 0.009*\"assange\" + 0.008*\"high\" + 0.007*\"yet\" + 0.007*\"human\"'),\n",
       " (7,\n",
       "  '0.021*\"draft\" + 0.018*\"speech\" + 0.015*\"send\" + 0.014*\"letter\" + 0.010*\"cdm\" + 0.010*\"message\" + 0.009*\"note\" + 0.009*\"ireland\" + 0.009*\"state\" + 0.007*\"statement\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display 10 words per topic as default\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the selected topics in one glance, we put in a list all the words defining a certain topic, we join them in a unique string and assign this one to a list called *topics*. The resulting topics are then printed by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1: secretary  office  room  state  conference  department  depart  private  arrive  route  time  en',\n",
       " 'Topic 2: russia  email  expeditionary  saddam  prevention  send  postconflict  limit  tony  traffic  dobbins  diplomacy',\n",
       " 'Topic 3: senate  obama  house  health  care  president  bill  vote  republican  time  start  year',\n",
       " 'Topic 4: sid  memo  thx  pis  good  hillary  ok  love  2010  list  pls  com',\n",
       " 'Topic 5: airport  laguardia  office  national  arrive  route  en  york  leahy  conf  astoria  treaty',\n",
       " 'Topic 6: percent  clinton  obama  2010  poll  president  voters  mr  election  opinion  democrats  time',\n",
       " 'Topic 7: haiti  cable  pakistan  report  sudan  letter  assange  high  yet  human  good  diplomats',\n",
       " 'Topic 8: draft  speech  send  letter  cdm  message  note  ireland  state  statement  clinton  thank',\n",
       " 'Topic 9: israel  israeli  right  peace  netanyahu  palestinian  bibi  jewish  party  group  obama  nuclear',\n",
       " 'Topic 10: mtg  air  andrews  base  house  force  mitchell  white  state  nations  unite  headquarter',\n",
       " 'Topic 11: state  2010  gov  b6  com  print  cheryl  clintonemail  pls  mill  hrod17  monday',\n",
       " 'Topic 12: thank  please  today  birthday  travel  tell  stone  good  autoreply  reach  confirm  nice',\n",
       " 'Topic 13: update  holbrooke  treaty  try  start  speech  defenses  leak  russian  gelb  ratification  dan',\n",
       " 'Topic 14: state  doc  2015  benghazi  case  information  subject  date  agreement  comm  05  sensitive',\n",
       " 'Topic 15: bloomberg  company  china  koch  climate  fund  change  per  praise  asia  editorials  financial',\n",
       " 'Topic 16: iran  obama  mr  american  b1  time  china  nuclear  tell  iranian  last  take',\n",
       " 'Topic 17: mayor  melanne  washington  verveer  dc  chicago  city  nw  amendment  district  brook  kurt',\n",
       " 'Topic 18: time  discuss  back  tomorrow  let  qddr  like  ask  happy  come  today  next',\n",
       " 'Topic 19: state  unite  diplomacy  government  force  security  american  world  president  support  people  war',\n",
       " 'Topic 20: party  labour  ok  david  cameron  miliband  tomorrow  election  move  members  sid  ed']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics here will be a list of strings\n",
    "no_words = 12 # number of words per topic to be printed\n",
    "topics = []\n",
    "for num in range(no_topics):\n",
    "    topic_prob = lda.show_topic(num,no_words)\n",
    "    topic = []\n",
    "    for word in range(len(topic_prob)):\n",
    "        topic.append(topic_prob[word][0])\n",
    "    topic = '  '.join(topic)\n",
    "    topics.append(('Topic '+str(num+1)+': '+topic))\n",
    "topics"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
