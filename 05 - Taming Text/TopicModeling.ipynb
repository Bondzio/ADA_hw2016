{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA\n",
    "\n",
    "The algorithm we want to apply to do topic modeling is known as **Latent Dirichlet Allocation** (for an exhaustive reference see [here](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)). This approach is mainly statistics-based, since a *Dirichlet prior* distribution is assumed for the training corpus. Multiple runs of the LDA on the same bunch of documents will provide slighlty different results, but once the final model is saved, its application to new corpora will be deterministic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import at first the cleaned body text from a csv file, build up during the previous sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hillary-clinton-emails/sentimentEmails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sentiment', 'SemiProcessedData', 'FullSemiProcessedData',\n",
       "       'ProcessedData'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we are interested in now is *ProcessedData*, where the cleaned textual information has been extracted. We check the eventual presence of NaN values in the dataset before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6453\n",
       "True        3\n",
       "Name: ProcessedData, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedData.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are NaNs left, we will wipe them out when building the corpus, as they cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define a list of stopwords to be wiped out from the documents; this words are typical English language stopwords or some trivial expressions, such as mail vocabulary tokens ('fw'), some numbers (which often prevent from a clear understanding of the underlying message), basic words ('get') and well as punctuation symbols. Numbers which are higher than 1899 represent likely years in dates and thus may contain useful information. Thereafter we define a list *documents* containing the split words of any text in *ProcessedData*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the documents from trivial recurrent words and from punctuation symbols\n",
    "\n",
    "punctuation_symbols = ['.',',',';',':','-','•','\"',\"'\",'?','!','@','#','/','*','+','(',')','—','{','}',\n",
    "                      '.\"',',\"','),','(,','<','>','%','&','$','---','----','-----','------','[',']',\n",
    "                      '■','--','...','://',').']\n",
    "trivial_words = ['us','fyi','fw','get']\n",
    "\n",
    "numbs = range(1900)\n",
    "numbers = [str(n) for n in numbs]\n",
    "numbers.insert(0,'00')\n",
    "\n",
    "stoplist = list(set(trivial_words).union(set(punctuation_symbols).union(set(numbers))))\n",
    "\n",
    "# apply the stoplist to each document in RawText\n",
    "documents = [[word for word in text.lower().split() if word not in stoplist and len(word)>1] # wipe out single letters\n",
    "            for text in df.ProcessedData.dropna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the *gensim* library and define a **dictionary**, which matches any word in each text with a numeric ID; notice that the documents are treated as *bows* (numeric vectors); the output of this operation is the **corpus** we will perform analysis on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a dictionary to associate ad Id to each token and build the corpus\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Latent Dirichlet Allocation using the dictionary and the corpus. The parameter *no_topics* defines the number of topics the algorithm must identify throughout the corpus. The higher it is, the more specific the returned topics will appear. Here we have chosen no_topics = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an lda model using the previously defined dictionary\n",
    "no_topics = 20\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics=no_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method of the LdaModel class allows to visualize the selected topics as a collection of (word,probability) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  '0.019*\"b1\" + 0.013*\"book\" + 0.011*\"mod\" + 0.009*\"thx\" + 0.008*\"beck\" + 0.007*\"state\" + 0.007*\"un\" + 0.005*\"bill\" + 0.004*\"email\" + 0.004*\"speech\"'),\n",
       " (10,\n",
       "  '0.011*\"ok\" + 0.005*\"like\" + 0.005*\"tell\" + 0.005*\"kabul\" + 0.004*\"still\" + 0.004*\"branch\" + 0.004*\"ask\" + 0.004*\"cable\" + 0.004*\"election\" + 0.004*\"change\"'),\n",
       " (15,\n",
       "  '0.016*\"office\" + 0.010*\"mayor\" + 0.009*\"qddr\" + 0.009*\"time\" + 0.008*\"bloomberg\" + 0.008*\"list\" + 0.007*\"secretary\" + 0.006*\"autoreply\" + 0.006*\"house\" + 0.006*\"white\"'),\n",
       " (5,\n",
       "  '0.029*\"state\" + 0.014*\"2010\" + 0.013*\"cheryl\" + 0.011*\"ops\" + 0.011*\"korea\" + 0.011*\"gov\" + 0.010*\"mill\" + 0.009*\"qddr\" + 0.009*\"secretary\" + 0.009*\"email\"'),\n",
       " (18,\n",
       "  '0.014*\"state\" + 0.009*\"department\" + 0.007*\"secretary\" + 0.006*\"pentagon\" + 0.005*\"defenses\" + 0.005*\"afghanistan\" + 0.005*\"fund\" + 0.005*\"american\" + 0.004*\"kennedy\" + 0.004*\"government\"'),\n",
       " (12,\n",
       "  '0.048*\"secretary\" + 0.041*\"office\" + 0.025*\"room\" + 0.023*\"state\" + 0.016*\"department\" + 0.016*\"arrive\" + 0.015*\"depart\" + 0.015*\"route\" + 0.015*\"en\" + 0.013*\"conference\"'),\n",
       " (8,\n",
       "  '0.015*\"haiti\" + 0.012*\"state\" + 0.010*\"b6\" + 0.007*\"2010\" + 0.007*\"please\" + 0.006*\"korean\" + 0.006*\"cdm\" + 0.006*\"traffic\" + 0.005*\"greek\" + 0.005*\"gov\"'),\n",
       " (4,\n",
       "  '0.008*\"diplomacy\" + 0.007*\"last\" + 0.006*\"bloomberg\" + 0.006*\"thank\" + 0.005*\"right\" + 0.005*\"time\" + 0.005*\"speech\" + 0.005*\"people\" + 0.005*\"year\" + 0.004*\"state\"'),\n",
       " (17,\n",
       "  '0.022*\"send\" + 0.013*\"state\" + 0.011*\"doc\" + 0.011*\"message\" + 0.010*\"tomorrow\" + 0.009*\"b5\" + 0.009*\"letter\" + 0.008*\"ashton\" + 0.007*\"blackberry\" + 0.007*\"pis\"'),\n",
       " (16,\n",
       "  '0.007*\"sid\" + 0.006*\"roger\" + 0.005*\"president\" + 0.005*\"another\" + 0.005*\"state\" + 0.005*\"2010\" + 0.005*\"involvement\" + 0.004*\"memo\" + 0.004*\"obama\" + 0.004*\"back\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display 10 words per topic as default\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the selected topics in one glance, we put in a list all the words defining a certain topic, we join them in a unique string and assign this one to a list called *topics*. The resulting topics are then printed by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic 1: party  labour  right  president  vote  support  election  david  politics  minister  time  last',\n",
       " 'Topic 2: b6  email  office  part  release  b1  please  thank  sid  travel  memo  send',\n",
       " 'Topic 3: israel  israeli  bibi  even  palestinian  state  diplomacy  palestinians  jewish  clinton  bill  settlements',\n",
       " 'Topic 4: development  women  deliver  strategy  message  press  conflict  last  yet  diplomacy  sid  today',\n",
       " 'Topic 5: 2010  state  blair  confidential  senate  may  party  tony  reason  declassify  b1  cameron',\n",
       " 'Topic 6: netanyahu  ok  yes  doc  state  greek  list  date  time  nazi  tomorrow  richards',\n",
       " 'Topic 7: afghanistan  qddr  taliban  state  pakistan  report  time  today  russia  security  confirm  issue',\n",
       " 'Topic 8: state  2010  defenses  henry  verveer  ambassador  fco  time  bomb  security  anne  per',\n",
       " 'Topic 9: state  secretary  office  time  today  branch  house  shuttle  staff  involvement  department  president',\n",
       " 'Topic 10: republican  republicans  2010  vote  percent  palin  democrats  voters  obama  party  election  gaza',\n",
       " 'Topic 11: secretary  office  room  arrive  state  route  en  department  depart  private  conference  residence',\n",
       " 'Topic 12: state  obama  american  people  government  unite  president  war  time  mr  force  world',\n",
       " 'Topic 13: 2010  israel  state  time  israeli  tell  schedule  palestinian  revcon  rob  saudi  land',\n",
       " 'Topic 14: state  boehner  treaty  cable  secretary  2010  reid  clinton  house  start  holbrooke  president',\n",
       " 'Topic 15: secretary  state  office  treaty  department  time  clinton  conference  president  press  foreign  obama',\n",
       " 'Topic 16: state  2010  tomorrow  gov  speech  huma  thank  schedule  lona  jake  tonight  morning',\n",
       " 'Topic 17: pls  happy  print  thx  good  bloomberg  discuss  pis  let  send  trip  day',\n",
       " 'Topic 18: state  gov  com  hrod17  2010  cheryl  clintonemail  mill  sullivanjj  b6  sullivan  millscd',\n",
       " 'Topic 19: haiti  care  health  bill  take  senators  like  prevention  back  un  traffic  give',\n",
       " 'Topic 20: leahy  list  kosovo  ca  conspiracy  stone  arturo  sid  like  theory  des  dinner']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topics here will be a list of strings\n",
    "no_words = 12 # number of words per topic to be printed\n",
    "topics = []\n",
    "for num in range(no_topics):\n",
    "    topic_prob = lda.show_topic(num,no_words)\n",
    "    topic = []\n",
    "    for word in range(len(topic_prob)):\n",
    "        topic.append(topic_prob[word][0])\n",
    "    topic = '  '.join(topic)\n",
    "    topics.append(('Topic '+str(num+1)+': '+topic))\n",
    "topics"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
