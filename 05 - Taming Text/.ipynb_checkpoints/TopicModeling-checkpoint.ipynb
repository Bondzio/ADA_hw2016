{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import at first the cleaned body text from a csv file, build up during the previous sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hillary-clinton-emails/sentimentEmails.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset consists of the following attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SentimentSIA', 'SentimentLHL', 'SemiProcessedData',\n",
       "       'FullSemiProcessedData', 'ProcessedData'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature we are interested in now is *ProcessedData*, where the cleaned textual information has been extracted. We check the eventual presence of NaN values in the dataset before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6453\n",
       "True        3\n",
       "Name: ProcessedData, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ProcessedData.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there are NaNs left, we will wipe them out when building the corpus, as they cannot be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we define a list of stopwords to be wiped out from the documents; this words are typical English language stopwords or some trivial expressions, such as mail vocabulary tokens ('fw', 're'), \"small\" numbers (which often prevent from a clear understanding of the underlying message), basic words ('get','would') and well as punctuation symbols. \"High\" numbers represent likely years in dates and thus may contain useful information. Thereafter we define a list *documents* containing the split words of any text in *ProcessedData*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear the documents from trivial recurrent words and from punctuation symbols\n",
    "\n",
    "punctuation_symbols = ['.',',',';',':','-','•','\"',\"'\",'?','!','@','#','/','*','+','(',')','—','{','}',\n",
    "                      '.\"',',\"','),','(,','<','>','%','&','$','---','----','-----','------','[',']',\n",
    "                      '■','--','...','://']\n",
    "trivial_words = ['u','w','h','j','us','fyi','would','fw','get']\n",
    "\n",
    "numbs = range(100)\n",
    "numbers = [str(n) for n in numbs]\n",
    "numbers = list(set(numbers).union(set(['00'])))\n",
    "\n",
    "stoplist = list(set(trivial_words).union(set(punctuation_symbols).union(set(numbers))))\n",
    "\n",
    "# apply the stoplist to each document in RawText\n",
    "documents = [[word for word in text.lower().split() if word not in stoplist]\n",
    "            for text in df.ProcessedData.dropna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the *gensim* library and define a **dictionary**, which matches any word in each text with a numeric ID; notice that the documents are treated as *bows* (numeric vectors); the output of this operation is the **corpus** we will perform analysis on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "# define a dictionary to associate ad Id to each token and build the corpus\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Latent Dirichlet Allocation using the dictionary and the corpus. The parameter *no_topics* defines the number of topics the algorithm must identify throughout the corpus. The higher it is, the more specific the returned topics will appear. Here we have chosen no_topics = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define an lda model using the previously defined dictionary\n",
    "no_topics = 20\n",
    "lda = models.ldamodel.LdaModel(corpus, id2word = dictionary, num_topics=no_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method of the LdaModel class allows to visualize the selected topics as a collection of (word,probability) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  '0.028*\"secretary\" + 0.022*\"office\" + 0.014*\"state\" + 0.013*\"room\" + 0.012*\"department\" + 0.009*\"time\" + 0.008*\"conference\" + 0.008*\"arrive\" + 0.008*\"private\" + 0.008*\"route\"'),\n",
       " (1,\n",
       "  '0.006*\"tea\" + 0.005*\"republicans\" + 0.004*\"obama\" + 0.004*\"leave\" + 0.004*\"back\" + 0.004*\"un\" + 0.004*\"like\" + 0.004*\"republican\" + 0.004*\"next\" + 0.004*\"thank\"'),\n",
       " (12,\n",
       "  '0.036*\"2010\" + 0.035*\"state\" + 0.035*\"gov\" + 0.016*\"com\" + 0.015*\"pls\" + 0.014*\"clintonemail\" + 0.014*\"b6\" + 0.013*\"cheryl\" + 0.013*\"hrod17\" + 0.011*\"mill\"'),\n",
       " (9,\n",
       "  '0.012*\"health\" + 0.012*\"care\" + 0.007*\"right\" + 0.006*\"statement\" + 0.006*\"today\" + 0.005*\"plan\" + 0.005*\"obama\" + 0.005*\"ok\" + 0.005*\"senators\" + 0.004*\"even\"'),\n",
       " (5,\n",
       "  '0.014*\"state\" + 0.006*\"force\" + 0.005*\"doc\" + 0.005*\"right\" + 0.005*\"unite\" + 0.005*\"diplomats\" + 0.004*\"2010\" + 0.004*\"world\" + 0.004*\"russia\" + 0.004*\"defenses\"'),\n",
       " (15,\n",
       "  '0.019*\"israel\" + 0.015*\"state\" + 0.014*\"israeli\" + 0.008*\"palestinian\" + 0.007*\"nuclear\" + 0.006*\"arab\" + 0.005*\"obama\" + 0.004*\"time\" + 0.004*\"president\" + 0.004*\"secretary\"'),\n",
       " (19,\n",
       "  '0.008*\"abu\" + 0.008*\"beck\" + 0.007*\"mazen\" + 0.006*\"russia\" + 0.005*\"clinton\" + 0.005*\"book\" + 0.005*\"bibi\" + 0.005*\"unite\" + 0.005*\"china\" + 0.004*\"germany\"'),\n",
       " (2,\n",
       "  '0.011*\"palin\" + 0.008*\"faith\" + 0.008*\"speak\" + 0.006*\"religious\" + 0.005*\"aipac\" + 0.005*\"speech\" + 0.005*\"religion\" + 0.005*\"framework\" + 0.005*\"candidate\" + 0.004*\"party\"'),\n",
       " (6,\n",
       "  '0.025*\"state\" + 0.011*\"boehner\" + 0.010*\"202\" + 0.009*\"647\" + 0.008*\"secretary\" + 0.008*\"department\" + 0.008*\"reid\" + 0.007*\"fco\" + 0.006*\"assistant\" + 0.006*\"lona\"'),\n",
       " (17,\n",
       "  '0.012*\"letter\" + 0.011*\"speech\" + 0.008*\"cable\" + 0.007*\"henry\" + 0.006*\"email\" + 0.006*\"print\" + 0.006*\"ask\" + 0.005*\"karzai\" + 0.005*\"good\" + 0.005*\"rich\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize the selected topics in one glance, we put in a list all the words defining a certain topic, we join them in a unique string and assign this one to a list called *topics*. The resulting topics are then printed by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : update  bloomberg  good  nice  thx  2010  trip  university  write  karl\n",
      "Topic 1 : tea  republicans  obama  leave  back  un  like  republican  next  thank\n",
      "Topic 2 : palin  faith  speak  religious  aipac  speech  religion  framework  candidate  party\n",
      "Topic 3 : obama  state  war  conflict  american  support  president  military  afghanistan  force\n",
      "Topic 4 : time  diplomacy  b1  obama  bill  b  mayor  leak  mr  travel\n",
      "Topic 5 : state  force  doc  right  unite  diplomats  2010  world  russia  defenses\n",
      "Topic 6 : state  boehner  202  647  secretary  department  reid  fco  assistant  lona\n",
      "Topic 7 : tomorrow  ops  thx  email  today  time  sid  ok  confirm  message\n",
      "Topic 8 : speech  draft  qddr  state  tomorrow  report  point  2010  send  usaid\n",
      "Topic 9 : health  care  right  statement  today  plan  obama  ok  senators  even\n",
      "Topic 10 : secretary  office  state  room  department  time  conference  arrive  private  route\n",
      "Topic 11 : mins  quick  stone  aid  jockey  afghan  propose  add  mtg  bono\n",
      "Topic 12 : 2010  state  gov  com  pls  clintonemail  b6  cheryl  hrod17  mill\n",
      "Topic 13 : women  time  right  take  much  tell  like  year  people  give\n",
      "Topic 14 : party  labour  mr  part  release  state  2010  poll  percent  david\n",
      "Topic 15 : israel  state  israeli  palestinian  nuclear  arab  obama  time  president  secretary\n",
      "Topic 16 : mod  love  branch  kabul  wjc  clinton  give  chapters  sudan  book\n",
      "Topic 17 : letter  speech  cable  henry  email  print  ask  karzai  good  rich\n",
      "Topic 18 : treaty  iran  ratification  colombia  missile  secretary  romney  iranian  ed  2010\n",
      "Topic 19 : abu  beck  mazen  russia  clinton  book  bibi  unite  china  germany\n"
     ]
    }
   ],
   "source": [
    "# topics here will be a list of strings\n",
    "topics = []\n",
    "for num in range(no_topics):\n",
    "    topic_prob = lda.show_topic(num)\n",
    "    topic = []\n",
    "    for word in range(len(topic_prob)):\n",
    "        topic.append(topic_prob[word][0])\n",
    "    topic = '  '.join(topic)\n",
    "    topics.append(topic)\n",
    "    \n",
    "for i,topic in enumerate(topics):\n",
    "    print('Topic',i,':',topic)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
