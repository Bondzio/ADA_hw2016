{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the needed modules\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests as req\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#request with Requests library, using Postman interceptor - empty IS-Academia form\n",
    "base_url = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportModel=133685247'\n",
    "r = req.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define BeautifulSoup object to parse the html doc\n",
    "soup = bs(r.text,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the HTML form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following the parameters model, section, academic year, academic cycle and season are parsed, so that each variable is linked to its corresponding numeric code through a dictionary. Thereafter, the parameters names are obtained as the IS_Academia form accepts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ww_i_reportModelXsl': '133685271', 'ww_i_reportmodel': '133685247'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the model code\n",
    "parse_model = soup.find_all('input')\n",
    "model = {}\n",
    "html_model = 'ww_i_reportmodel'\n",
    "xls_model = 'ww_i_reportModelXsl'\n",
    "for ind in parse_model:\n",
    "    if ind['name']==html_model:\n",
    "        model[html_model]=ind['value']\n",
    "    if ind['name']==xls_model:\n",
    "        model[xls_model]=ind['value']\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Informatique': '249847'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the section Informatique\n",
    "section = {'Informatique' : soup.find('option',string='Informatique')['value']}\n",
    "section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2007-2008': '978181',\n",
       " '2008-2009': '978187',\n",
       " '2009-2010': '978195',\n",
       " '2010-2011': '39486325',\n",
       " '2011-2012': '123455150',\n",
       " '2012-2013': '123456101',\n",
       " '2013-2014': '213637754',\n",
       " '2014-2015': '213637922',\n",
       " '2015-2016': '213638028',\n",
       " '2016-2017': '355925344'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the academic year - loop over the years and find the code corresponding to the year string\n",
    "academic_year = {}\n",
    "for year in range(2007,2017):\n",
    "    period = str(year)+'-'+str(year+1)\n",
    "    academic_year[period] = soup.find('option', string = period)['value']\n",
    "academic_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-05a5f0537f14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0macademic_cycle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcycle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbachelor_master\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0macademic_cycle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcycle\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'option'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcycle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0macademic_cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# parse the academic cycle - define the sought cycles and look for the corresponding codes, as above\n",
    "bachelor = []\n",
    "master = []\n",
    "for j in range(1,7):\n",
    "    bachelor.append('Bachelor semestre '+str(j))\n",
    "for j in range(1,5):\n",
    "    master.append('Master semestre '+str(j))\n",
    "bachelor_master = bachelor+master\n",
    "\n",
    "# add the master projects\n",
    "bachelor_master.append(['Projet Master automne', 'Projet Master printemps'])\n",
    "\n",
    "academic_cycle = {}\n",
    "for cycle in bachelor_master:\n",
    "    academic_cycle[cycle] = soup.find('option', string = cycle)['value']\n",
    "academic_cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Semestre d'automne\": '2936286', 'Semestre de printemps': '2936295'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the academic season - same procedure as above\n",
    "season = {}\n",
    "season[\"Semestre d'automne\"]=soup.find('option', string = \"Semestre d'automne\")['value']\n",
    "season['Semestre de printemps']=soup.find('option', string = 'Semestre de printemps')['value']\n",
    "season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ww_x_UNITE_ACAD',\n",
       " 'ww_x_PERIODE_ACAD',\n",
       " 'ww_x_PERIODE_PEDAGO',\n",
       " 'ww_x_HIVERETE']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the variables' names to fill in the IS-Academia form\n",
    "variables = []\n",
    "args = soup.find_all('select')\n",
    "for var in args:\n",
    "    variables.append(var['name'])\n",
    "variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction of the GPS identifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now proceed to the extraction of the html tables from IS-Academia. The previous dictionaries are used to pass the correct parameters to send the request. The crucial information to be retrieved is the parameter 'ww_x_GPS', which identifies the table to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a general dictionary to perform the search\n",
    "research_keys = {}\n",
    "research_keys[variables[0]] = section['Informatique']\n",
    "research_keys[variables[1]] = academic_year['2013-2014']\n",
    "research_keys[variables[2]] = academic_cycle['Bachelor semestre 1']\n",
    "research_keys['ww_b_list'] = 1 # variable to get the tables through the GPS parameter\n",
    "research_keys['ww_x_HIVERETE'] = 'null'\n",
    "research_keys[html_model] = model[html_model]\n",
    "research_keys[xls_model] = model[xls_model]\n",
    "r0 = req.get(base_url, params = research_keys)\n",
    "s0 = bs(r0.text,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we define a suitable function to \"hack\" the value of the variable ww_x_GPS. Let us consider the following example:\n",
    "\n",
    "class=\"ww_x_GPS\" href=\"javascript:void(0)\" onclick=\"loadReport('ww_x_GPS=1378362120');return false;\">Informatique, 2012-2013, Bachelor semestre 2\n",
    "\n",
    "The GPS code is reported within the call to the html function 'loadReport', which is called when clicking the dataset. The function captureGPS seeks for the equal sign '=' inside the round parentheses and stops before the apostrophe sign. Once got the value of ww_x_GPS, one can use it to perform a request to the IS-Academia server passing a proper base url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to detect the GPS code of the table\n",
    "# strong assumption of the string's structure!\n",
    "def captureGPS(string):\n",
    "    letter = 0\n",
    "    while(string[letter]!='='):\n",
    "        letter=letter+1 #advance up to the =\n",
    "    letter = letter+1 # reach the first number of the GPS code\n",
    "    GPS = string[letter]\n",
    "    letter = letter+1\n",
    "    while (string[letter]!=\"'\"):\n",
    "        GPS = GPS+string[letter]\n",
    "        letter = letter+1\n",
    "    return GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GPS = 'ww_x_GPS'\n",
    "# any (uniquely indentified) search output is made up by 2 tables: the 'Tous', which corresponds to ww_x_GPS = -1, and the table\n",
    "# we actually want to extract. The method find_all returns this table as the second one in the list, so that we access it as\n",
    "# soup.find_all('a')[1]\n",
    "research_keys[GPS]=captureGPS(s0.find_all('a')[1]['onclick'])\n",
    "request_url = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?'\n",
    "r0 = req.get(request_url, params = research_keys)\n",
    "s0 = bs(r0.text,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a single dataframe (only ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Civilité',\n",
       " 'Nom Prénom',\n",
       " 'Orientation Bachelor',\n",
       " 'Orientation Master',\n",
       " 'Spécialisation',\n",
       " 'Filière opt.',\n",
       " 'Mineur',\n",
       " 'Statut',\n",
       " 'Type Echange',\n",
       " 'Ecole Echange',\n",
       " 'No Sciper']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the attributes of the dataframe to be created\n",
    "attributes_list = s0.find_all('th')[2:]\n",
    "attributes = []\n",
    "for ind in attributes_list:\n",
    "    attributes.append(ind.contents[0])\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build up the dataframe\n",
    "data = pd.read_html(r0.url)\n",
    "data = data[0][3:] # data is a list of dataframes of length 1; remove the first 2 rows\n",
    "del data[11]\n",
    "del data[12]\n",
    "data.columns = attributes\n",
    "data = data.set_index('Nom Prénom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add information to the dataset: academic year, academic cycle\n",
    "data['Période académique']='Bachelor semestre 1'\n",
    "data['Année académique']='2013-2014'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the tables from IS-Academia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin now the importation of all the tables from the server. We define lists containing the values of the required parameters (section, year, cycle) and then seek for the ww_x_GPS parameter of each single table. We thus define a dataframe object and append progressively the datasets with the methods provided by the Pandas library. The index of the resulting dataframe will be a combination of name and Sciper number, which is of course not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the parameters to perform the requests\n",
    "year = []\n",
    "for j in range(2007,2017):\n",
    "    year.append(str(j)+'-'+str(j+1))\n",
    "\n",
    "# the periods list has already been defined during the parsing operations\n",
    "#bachelor_master\n",
    "\n",
    "course = 'Informatique'\n",
    "\n",
    "# model variables have already been defined\n",
    "# html_model, xls_model\n",
    "\n",
    "dataset = pd.DataFrame(); #empty dataframe where to append all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year: 2007-2008 parsed\n",
      "Finished parsing\n"
     ]
    }
   ],
   "source": [
    "# for now, only one year\n",
    "request_url = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.html?'\n",
    "base_url = 'http://isa.epfl.ch/imoniteur_ISAP/!GEDPUBLICREPORTS.filter?ww_i_reportModel=133685247'\n",
    "\n",
    "subyear = year[0:1]\n",
    "\n",
    "for cur_year in subyear:\n",
    "    for cycle in bachelor_master: # loop over the bachelor and master cycles\n",
    "        # define a search dictionary using the previous variables\n",
    "        research_keys = {}\n",
    "        research_keys[variables[0]] = section[course]\n",
    "        research_keys[variables[1]] = academic_year[cur_year]\n",
    "        research_keys[variables[2]] = academic_cycle[cycle]\n",
    "        research_keys['ww_b_list'] = 1\n",
    "        research_keys['ww_x_HIVERETE'] = 'null' # not important to extract the season - linked to the cycle\n",
    "        research_keys[html_model] = model[html_model]\n",
    "        research_keys[xls_model] = model[xls_model]\n",
    "        \n",
    "        r0 = req.get(base_url, params = research_keys)\n",
    "        s0 = bs(r0.text,'lxml') # beautifulsoup object from where to extract the GPS parameter\n",
    "        \n",
    "        # check for correctness of the call to captureGPS - missing data might occur\n",
    "        if(len(s0.find_all('a')) > 0):\n",
    "            research_keys[GPS]=captureGPS(s0.find_all('a')[1]['onclick'])\n",
    "            r = req.get(request_url, params = research_keys)\n",
    "            s = bs(r.text,'lxml')\n",
    "    \n",
    "            # s is the beautifulsoup object pointing to the table where are interested in\n",
    "            # define the columns names\n",
    "            attributes_list = s.find_all('th')[2:]\n",
    "            attributes = []\n",
    "            for ind in attributes_list:\n",
    "                attributes.append(ind.contents[0])\n",
    "            \n",
    "            # import the dataframe and manipulate properly the layout\n",
    "            df = pd.read_html(r.url)\n",
    "            df = df[0][3:] # data is a list of dataframes of length 1; remove the first 2 rows ad the last 2 empty columns\n",
    "            del df[11]\n",
    "            del df[12]\n",
    "            df.columns = attributes\n",
    "            df = df.set_index('No Sciper')\n",
    "            \n",
    "            # add year and cycle information\n",
    "            df['Période académique']=cycle\n",
    "            df['Année académique']=cur_year\n",
    "    \n",
    "            # append dataframe\n",
    "            dataset = dataset.append(df)\n",
    "        \n",
    "    # useful for the user to understand the underlying process\n",
    "    message = 'year: '+str(cur_year)+' parsed'\n",
    "    print(message)\n",
    "    \n",
    "print('Finished parsing')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
